# ğŸ‘©ğŸ»â€ğŸ’» Raul Ferreira

**Engenheiro de Dados**

Sou um amante de tecnologia desde cedo, que aproveitou a oportunidade para sair da Ã¡rea de sucesso do cliente e me apaixonar por dados.

Atualmente busco empoderar com dados as tomadas de decisÃ£o das Ã¡reas de negÃ³cio, centralizando informaÃ§Ãµes das ferramentas usadas por todos os setores em um datalake. Isso otimiza e facilita o trabalho dos cientistas e analistas de dados na criaÃ§Ã£o de dashboards que auxiliam os stakeholders.

Utilizo todo o processo de ETL para transformar dados brutos em dados tratados, prontos para uso na camada ouro, com ferramentas como **Apache Airflow**, **PySpark** e **Python**.

---

## ğŸ‘¤ Perfil

- **Nome:** Raul dos Santos Ferreira  
- **Data de nascimento:** 06 de janeiro, 1994  
- **ProfissÃ£o:** Engenheiro de Dados, Analista de Dados  
- **PortuguÃªs:** Nativo  
- **InglÃªs:** RazoÃ¡vel

---

## ğŸ› ï¸ Linguagens e Tecnologias

- **SQL**  
- **Python**  
- **Apache Spark**  
- **Apache Airflow**  
- **Google Cloud Platform (GCP)**  
- **AWS**  
- **Linux / Windows**

---

## ğŸš€ Skills

NÃ­veis de proficiÃªncia com tecnologias:

| Tecnologia              | NÃ­vel       |
|-------------------------|-------------|
| Apache Airflow          | Muito bom   |
| Python                  | Muito bom   |
| Apache Spark            | Muito bom   |
| Google Cloud Platform   | Muito bom   |
| SQL                     | Muito bom   |
| AWS                     | Bom         |
| Scrum                   | Muito bom   |

---

## ğŸ’¼ ExperiÃªncias

### ğŸ¢ Aurum Software Brasil
- **Cargo:** Analista de Dados Pleno  
- **PerÃ­odo:** 01/2024 â€“ Atual  
- **Atividades:**  
  - CriaÃ§Ã£o de DAGs para extraÃ§Ã£o e transformaÃ§Ã£o de dados (batch e streaming)
  - Ferramentas: Apache Airflow, Apache Spark, Python, GCP, BigQuery
  - Suporte Ã  criaÃ§Ã£o de dashboards (Power BI e Looker Studio)
  - Monitoramento e escalabilidade do ambiente GCP
  - AutomaÃ§Ã£o com Cloud Functions e Cloud Scheduler

### ğŸ¢ Grupo PÃ£o de AÃ§Ãºcar (via Orange Fox)
- **Cargo:** Engenheiro de Dados Pleno  
- **PerÃ­odo:** 02/2025 â€“ Atual  
- **Atividades:**  
  - ExecuÃ§Ã£o e monitoramento de rotinas em GCP (Composer) e Teradata (mainframe IBM)
  - Garantia de SLA e migraÃ§Ã£o do ambiente Teradata para GCP

### ğŸ¢ Aurum Software Brasil
- **Cargo:** Analista de NegÃ³cios Pleno  
- **PerÃ­odo:** 01/2022 â€“ 01/2024  
- **Atividades:**  
  - GestÃ£o de demandas do time de tecnologia
  - AplicaÃ§Ã£o de metodologia Scrum (dailys, plannings)
  - DocumentaÃ§Ã£o e definiÃ§Ã£o de SLAâ€™s
  - Roadmaps de melhoria e alinhamento com clientes

---

## ğŸ“ FormaÃ§Ã£o AcadÃªmica

- **Curso:** CiÃªncia da ComputaÃ§Ã£o  
- **InstituiÃ§Ã£o:** Universidade Nove de Julho  
- **ConclusÃ£o:** 12/2015

---

## ğŸŒŸ PortfÃ³lio

ConheÃ§a meus principais projetos. Fique Ã  vontade para explorar e me chamar caso queira conversar sobre algum deles!

---

### ğŸ“Œ **Aurum Datapipeline**

**Desafio:**  
Desenvolver do zero um **Data Lake** que centralizasse os dados de todas as ferramentas da empresa, promovendo uma cultura data-driven.

**SoluÃ§Ã£o implementada:**  
- OrquestraÃ§Ã£o com **Apache Airflow**
- TransformaÃ§Ã£o com **Apache Spark**
- Linguagem **Python**
- Infraestrutura **Google Cloud Platform**

**Objetivo:**  
Centralizar dados de CRM, CS, finanÃ§as, produtos e sistemas prÃ³prios. Criar uma arquitetura escalÃ¡vel e disponÃ­vel para aumentar a performance na entrega de dados.

#### ğŸ”§ ConfiguraÃ§Ã£o do ambiente GCP
- CriaÃ§Ã£o do projeto GCP e ativaÃ§Ã£o das APIs:
  - Cloud Composer API
  - Cloud Storage API
  - Compute Engine API
  - Cloud SQL Admin API
- CriaÃ§Ã£o dos buckets (raw, processing, curated) e estrutura do Composer
- IntegraÃ§Ã£o com GitHub para atualizaÃ§Ã£o automÃ¡tica via pull request (feita pela equipe de DevOps)

#### ğŸ§© Desenvolvimento da Pipeline
- **Hooks:** conexÃ£o com as APIs e webhooks
- **Operators:** criaÃ§Ã£o dos diretÃ³rios e armazenamento no GCS
- **Transformations:** `transformation.py` (camada prata) e `resume.py` (camada ouro)
- **DAGs:** orquestraÃ§Ã£o das etapas e carga no BigQuery

#### ğŸ”Œ Fontes de dados integradas

Captura e transformaÃ§Ã£o de dados das seguintes plataformas:

- [Aha!](https://www.aha.io/)
- [Astrea](https://www.astrea.net.br/)
- [Beamer](https://www.getbeamer.com/)
- [Microsoft Ads](https://ads.microsoft.com/)
- [Circle](https://circle.so/)
- [Google Ads](https://ads.google.com/)
- [Intercom](https://www.intercom.com/)
- [LinkedIn Ads](https://business.linkedin.com/marketing-solutions/ads)
- [Meta Ads](https://www.facebook.com/business/ads)
- [Mixpanel](https://mixpanel.com/)
- [Pipedrive](https://www.pipedrive.com/)
- [Pipefy](https://www.pipefy.com/)
- [RD Station](https://www.rdstation.com/)
- [Sensedata](https://sensedata.com.br/)

AlÃ©m disso, tratamos dados de sistemas internos como:
- Plataforma financeira interna
- RobÃ´s e automaÃ§Ãµes proprietÃ¡rias

---
## ğŸ¤ Entre em contato

Caso tenha dÃºvidas sobre algum dos projetos ou queira conversar sobre tecnologia e dados, estou Ã  disposiÃ§Ã£o!

---
## ğŸ“« Contato

- ğŸ“§ **E-mail:** raul.engdados@gmail.com  
- ğŸ’¼ **LinkedIn:** [Raul dos Santos Ferreira](https://www.linkedin.com/in/raul-santos-ferreira/)

---
